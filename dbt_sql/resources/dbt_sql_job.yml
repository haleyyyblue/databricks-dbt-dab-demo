resources:
  jobs:
    dbt_sql_job:
      name: dbt_sql_job

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      email_notifications:
        on_failure:
          - haekyung.won@databricks.com

      tasks:
        - task_key: dbt
          environment_key: Default
          dbt_task:
            catalog: haley_source_ws
            schema: default
            project_directory: ../
            # The default schema, catalog, etc. are defined in ../dbt_profiles/profiles.yml
            warehouse_id: 61b2ae8974016fcd
            # profiles_directory: dbt_profiles/
            commands:
            # The dbt commands to run (see also dbt_profiles/profiles.yml; dev_schema is used in the dev profile)
            - 'dbt deps'
            - 'dbt seed'
            - 'dbt run'
          # libraries:
          # - pypi:
          #     package: dbt-databricks>=1.8.0,<2.0.0

          # new_cluster:
          #   spark_version: 15.4.x-scala2.12
          #   node_type_id: i3.xlarge
          #   data_security_mode: SINGLE_USER
          #   num_workers: 0
          #   spark_conf:
          #       spark.master: "local[*, 4]"
          #       spark.databricks.cluster.profile: singleNode
          #   custom_tags:
          #     ResourceClass: SingleNode
      environments:
        - environment_key: Default
          spec:
            environment_version: "1"
            dependencies:
              - dbt-databricks>=1.8.0,<2.0.0

        
